%!TEX root = ../main.tex
\section{Deferred Implementation Details}
\label{appendix:implementation}
Here, we provide the details for our optimization techniques.
\paragraph{Bit packing for $\Z_2$ vectors.} 
Instead of representing each element in a $\Z_2$ vector separately, we pack several elements into a machine word and operate on them together in an SIMD manner. For our architecture with 64-bit machine words, we can pack a vector in $\Z_2^{256}$ (e.g., the input $x$) into 4 words. Since the key $\mat{K}$ is circulant and can be represented with $n=256$ bits, it can also be represented by 4 words. This results in a theoretical $\times64$ maximum speedup in run-time for operations involving $\mat{K}$ and $x$. 

\paragraph{Bit slicing for $\Z_3$ vectors.}
We represent each element in $\Z_3$ using the two bits from its binary representation. For $z \in \Z_3$, the two bits are the least significant bit (LSB) $l_z = z \bmod 2$, and the most significant bit (MSB) $h_z$ which is $1$ if $z=2$ and $0$ otherwise. $\Z_3$ vectors are now also represented by two binary vectors, one containing the MSBs, and one containing the LSBs. Operations involving a $\Z_3$ vector are translated to operations on these binary vectors instead. We also take advantage of the bit packing optimization when operating on the binary vectors.
In Table~\ref{table:z3_operations}, we specify how we perform common operations on $\Z_3$ elements using our bit slicing approach. 

\begin{table}[ht]
\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        Operation & Result MSB & Result LSB\\
        \hline\hline
        $z_1 + z_2 \bmod 3$ & $(l_1 \vee l_2) \oplus (l_1 \vee h_2) \oplus (l_2 \vee h_1)$  & $(h_1 \vee h_2) \oplus (l_1 \vee h_2) \oplus (l_2 \vee h_1)$\\
        $-z_1 \bmod 3$ & $l_1$ & $h_1$ \\
        $z_1z_2 \bmod 3$ & $(l_1 \wedge l_2) \oplus (h_1 \wedge h_2)$ & $(l_1 \wedge h_2) \oplus (h_1 \wedge l_2)$\\
        $\textrm{MUX}(s;z_1,z_2)$ & $(h_2 \wedge s) \vee (h_1 \wedge \neg s)$ & $(l_2 \wedge s) \vee (l_1 \wedge \neg s)$\\
        \hline
    \end{tabular}
\end{center}
\caption{Operations in $\Z_3$. $z_1$ and $z_2$ are elements in $\Z_3$ with (MSB, LSB) = $(h_1,l_1)$ and $(h_2, l_2)$ respectively. For a bit $s$, the operation $\textrm{MUX}(s;z_1, z_2)$ outputs $z_1$ when $s=0$ and $z_2$ when $s=1$.}
\label{table:z3_operations}
\mahimna{I'm debating whether we need to have this table at all. Once we specify that we split $\Z_3$ elements into 2 bits, for any $\Z_3$ operation, the corresponding operation on the bits can easily be computed using a truth table.}
\end{table}



\paragraph{Lookup table for matrix multiplication.} Recall that the \ttwPRF evaluation contains a mod-3 linear map using a public matrix $\mat{B} \in \Z_3^{81\times256}$. Specifically, it computes the matrix-vector product $\mat{B}w$ where $w \in \Z_3^{256}$. Since $\mat{B}$ is known prior to evaluation, we can use a lookup table to speedup the multiplication by $\mat{B}$\cite{arlazarov1970economical}. The same preprocessing can also by reused for multiple evaluations of the wPRF.

For this, we partition $\mat{B}$, which has $m=256$ columns, into $16$ slices of $16$ columns each. These matrices, denoted by $\mat{B}_1,\dots, \mat{B}_{16}$, are all in $\Z_3^{81 \times 16}$. Now, for each $\mat{B}_i$, we will effectively build a lookup table for its multiplication with any $\Z_3$ vector of length $16$. A point to note here is that since we  represent $\Z_3$ vectors by two binary vectors (from the bit slicing optimization), it is sufficient to preprocess multiplications (modulo 3) for binary vectors of length $16$. To multiply $\mat{B}_{i}$ by a vector in $\Z_3^{16}$, we can first multiply it separately by the corresponding MSB and LSB vectors, and then subtract the former from the latter modulo 3. This works since for $z_1, z_2 \in \Z_3$ the multiplication $z_1z_2 \bmod 3$ can be given by $z_1(2\cdot h_2 + l_2) \bmod 3 = z_1l_2 - z_1h_2 \bmod 3$ where $h_2, l_2$ are the MSB and LSB of $z_2$ respectively. Now, to multiply $\mat{B}$ by $v \in \Z_3^{256}$, we first evaluate all multiplications of the form $\mat{B}_iv_i$ where $v_i$ is the $\Z_3^{16}$ vector denoting the $i^\thtext$ slice of $v$ if it was split into 16-element chunks. Then, multiplication by $\mat{B}$ is given by $\mat{B}v = \sum_{i \in [16]} \mat{B}_i v_i \mod 3$.
\greg{Is this the ``method of four Russians''?\url{https://github.com/malb/m4ri}, \url{https://bitbucket.org/malb/m4ri/wiki/Further\%20Reading} }
\greg{$w$ is secret, so the access pattern of the $\mat{B}_i$ should be independent of $w$. Is this the case? If not there is a 
cache timing side-channel. The timing variation may be observable over the network (e.g., to the other party in the OPRF evaluation).  }

In general, a $\mat{B} \in \Z_3^{t \times m}$ can be partitioned into $m/c$ partitions with $c$ columns each (assume $c$ divides $m$ for simplicity), and would require a total multiplication lookup table size of $(m/c) \cdot 2^c \cdot t$ $\Z_3$ elements. Multiplying $\mat{B}$ by $v \in \Z_3^{m}$ requires $2(m/c)$ lookup table accesses (one each for MSB and LSB of $v$ per partition of $\mat{B}$) and an addition of $(m/c - 1)$ $\Z_3^{t}$ vectors.


For our parameters, this results in a table size of roughly $135$MB with proper $\Z_3$ packing. We chose $c = 16$ as a compromise between the size of the lookup table and increased computational efficiency.