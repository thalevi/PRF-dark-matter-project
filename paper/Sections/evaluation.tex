%!TEX root = ../main.tex

\iffalse
\section{Experiments and Evaluation}

\paragraph{Experimental setup.}

\paragraph{Optimizations.}
e.g., preprocessing table, bit packing etc.

\paragraph{Core primitive benchmarks.}

\paragraph{Centralized vs distributed benchmarks.}

\paragraph{Comparison to existing approaches.}

\fi


\newpage
\section{Implementation and Evaluation}
\label{sec:implementation_and_eval}
We implemented our 2-party 23-wPRF (Construction~\ref{construction:23-central-wprf}) and 23-OPRF (Construction~\ref{construction:23-owf})  in C++. For the constructions, we used the parameters $n = m = 256$ and $t = 81$. In other words, the implemented 23-constructions use a circulant matrix in $\Z_2^{256 \times 256}$ as the key, take as input a vector in $\Z_2^{256}$ and output a vector in $\Z_3^{81}$. The correlated randomness was implemented as if provided by a trusted third party.  See Section~\ref{to-add} for concretely efficient protocols for securely generating the correlated randomness, which we did not implement but give efficiency estimates based on prior works.  \yuval{Add pointer when section is written.}

\subsection{Optimizations}
\label{subsec:implementation_opt}
We start with a centralized implementation of the 23-wPRF. We find optimizations that provide 5x better performance over a na\"ive implementation, which we detail below. Later, in Table~\ref{table:optimization_benchmarks}, we compare the performance gain from these optimizations.

\subsection{Representing $Z_2$ vector - bit packing}

The dark matter function manipulates elements of $Z_2$ and $Z_3$. However, machine words can hold up to 64 bits and we use this to represent many elements in just a few machine words, using bit slicing. Namely, we used each machine word as a vector of 64 bits and applied operations to this bit-vector in a SIMD manner.
Since in our implementation each key $\textbf{K}$  is of size $m \times n = 256 \times 256$ and each input is a vector of size $n = 256$, this results in packing each key into $\dfrac {256}{64} = 4$ words and each input vector into 4 words. This may result in time saving of up to $\times 64$ of the run-time.

\subsection{Representing $Z_3$ vector - bit slicing}

The last part of the dark matter function takes an intermediate vector viewed as a vector of $Z_3$ elements, which is then multiplied 
by a matrix $R$ in $Z_3$ to produce the output vector in $Z_3$. We tried different methods for implementing this matrix-vector multiplication over $Z_3$.:	

Bit slicing is implemented  by representing a vector over $Z_3$  as two binary vectors - one LSB's and one MSB's. Since each matrix has 81 columns, each vector is represented by two machine words, for a total of 4 words per $Z_3$ element. The operations over this $Z_3$ vector - addition, subtraction, multiplication and MUX  - were implemented in a bitwise manner as shown in table \ref{tab:multicol}. We took advantage of the fact that when representing each vector as MSB and LSB, negation (which is the same as multiplying by two) is the same as swapping the most and least significant bits. 

\begin{table}[ht]
	\caption{Operations in $Z_3$. input:   ${l_1}, {m_1}$ - LSB and MSB  of first trinary number, 
		${l_2}, {m_2}$ - LSB and MSB  of second trinary number,  ${s}$ - selection bit for MUX}
	\begin{center}
		\begin{tabular}{|c|l|}
			\hline
			\textbf{Operations} & \textbf{Methods}\\
			\hline
			\multirow{3}{*}{Addition} & ${t} := ({l_1 \wedge m_2}) \oplus ({l_2 \wedge m_1})$\\
			& $m_{\mathrm{out}} := ( l_1 \wedge  l_2 ) \oplus  t $ \\
			& $l_{\mathrm{out}} :=m_1 \wedge m_2 ) \oplus t $ \\
			\hline
			\multirow{3}{*}{Subtraction} & ${t} := ({l_1} \wedge {l_2}) \oplus ({m_2} \wedge {m_1})$;\\
			& $m_{\mathrm{out}} := (l_1 \wedge m_2 ) \oplus t$;\\
			& $l_{\mathrm{out}} := (m_1 \wedge l_2 ) \oplus t$; \\
			\hline
			\multirow{3}{*}{Multiplication} & $m_{\mathrm{out}}:= (l_1 \vee m_2) \wedge   (m_1 \vee l_2)$; \\
			& $l_{\mathrm{out}} := (l_1 \vee l_2) \wedge     (m_1 \vee m_2)$;\\
			\hline
			\multirow{3}{*}{MUX} & $m_{\mathrm{out}} :=( m_2 \vee s) \wedge (m_1 \vee (\bar{s}) )$; \\
			& $l_{\mathrm{out}}[i] :=( l_2 \vee s) \wedge (l_1 \vee (\bar{s}) )$; \\
			\hline
			
		\end{tabular}
	\end{center}
	\label{tab:multicol}
\end{table}

%Lookup table Implemetation
\paragraph{Lookup table for matrix multiplication.}
Even with bitslicing, implementing the matrix-vector multiplication column by column via the opertions from Table~1 is still rather slow.
To get better performence, we capitalized on the fact that the random matrix $R$ is fixed, and we can therefore set up $R$-dependent tables and use table lookups in the implementation of the multiply-by-$R$ operation.

Specifically, we partition the matrix $R$ (which has $m=256$ columns) into sixteen slices of 16 columns each, denoted $R_1,\ldots,R_{16}$.
For each of these small matrices $R_i$, we then build a table with $2^{16}$ entries, holding the result of multiplying $R_i$ by every vector from $\{0,1\}^{16}$.
Namely, let $R_{i,0},R_{i,1},\ldots,R_{i,15}$ be the sixteen columns of the matrix $R_i$.
The table for $R_i$ is then defined as follows: For each index $0\le j < 2^{16}$, let $\vec{J}=(j_0,j_1,\ldots,j_{15})$ be the 0-1 vector holding the bits in the binary representation of~$j$, then we have
\[
T_i[j] = R_i \times \vec{J} = \sum_{k=0}^{15} R_{i,k} \cdot j_k ~~\bmod 3.
\]
Recalling that $R$ is $Z_3$ matrix of dimension $81\times 256$, every entry in each table~$T_i$ therefore holds an 81-vector over $Z_3$.
Specifically, it holds a packed-$Z_3$ element with four words (two for the MSBs and two for the LSBs of this 81-element vector).

Note, however, that $T_i$ can only be used directly to multiply $R_i$ by \emph{0-1 vectors}.
To use $T_i$ when multiplying $R_i$ by a $\{0,1,2\}$ vector, multiply $R_i$ separately by the MBSs and the LSBs of that vector, and then subtract one from the other using the operations from Table~1.
To multiply a dimension-256 $\{0,1,2\}$ vector by the matrix $R$, we partition it into sixteen vectors of dimension 16, use the approach above to multiply each one of them by the corresponding $R_i$, then use the operations from Table~1 to add them all together.
Hence we have

\begin{tabbing}
	\underline{Multiply-by-$R$(input: $\vec{V}\in\{0,1,2\}^{256}$:}\\
	~~1. $Acc := 0$\\
	~~2. For \=$i=0,\ldots,15$\\
	~~3. \> Let $\vec{M}_i,\vec{L}_i\in\{0,1\}^{16}$ be the MSB, LSB vectors (resp.) of $\vec{V}_i=\vec{V}[16i,\ldots,16i+15]$\\
	~~4. \> Set the indexes $m:=\sum_{k=0}^{15}2^i\cdot\vec{M}[i]$
	and $\ell:=\sum_{k=0}^{15}2^i\cdot\vec{L}[i]$\\
	~~5. \> $Acc := Acc + T_i[\ell] - T_i[\ell] \bmod 3$\\
	~~6. Output $Acc$
\end{tabbing}

We note that a slightly faster implementation could be obtained by braking the matrix into (say) 26 slices of upto 10 columns each, and directly identify each 10-vector over $\{0,1,2\}$ with an index $i<3^{10}=59049$.
This implementation would have only 26 lookup operations instead of 32 in the algorithm above, so we expect it to be about 20\% faster. On the other hand it would have almost twice the table size of the implementation from above.


\subsection{Performance Benchmarks}
\paragraph{Experimental setup}
We ran all our experiments on a t2.medium AWS EC2 instance with 4GiB RAM (architecture: x86-64 Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz) running on Ubuntu 18.04. The performance benchmarks and timing results we provide are averaged over 1000 runs. For the distributed construction benchmarks, both parties were run on the same instance. We separately report the computational runtime for the parties, and provide an estimate of the communication costs. 

Communication vs. Computation: from the table we can estimate the break even point for each protocol. Note that synchronization is needed between the parties, though, so the assumption this is independent is an estimate. For the  wPRF protocol \ref{newProtocol} presented in this paper, the break even point is achieved  at a network speed 63 Mbps, in which case the overall communication and computation times will be the same. For the oPRF protocol, the break even point will be achieved with a network of speed 146 Mbps. 
From here we see that for the oPRF protocol, for an existing basic 25 Mbps connection, communication will dominate the total run-time, while for a high speed network of 1 Gbps, the computation time will dominate the overall run-time.

\yuval{Communication costs in bits can be computed analytically. Maybe talk about estimated breakeven points: (1) minimal network speed in which computation dominates communication, and (2) minimal network speed where we estimate our OPRF to be faster than the DDH-based alternative.  }

\paragraph{Optimization results.}
We start with benchmarks for the different optimization techniques detailed in Section~\ref{subsec:implementation_opt}. Table~\ref{table:optimization_benchmarks} contains timing results for our centralized implementation of the 23-wPRF construction using these optimizations.

\begin{table}[h]
	{
		\centering
		\begin{tabular}{|c|c|c|c|c|}
			
			\hline
			\multicolumn{3}{|c|}{Optimization} &  \multirow{2}{*}{Runtime($\mu$ sec)} & \multirow{2}{*}{Evaluations / sec} \\
			Packing & Bit Slicing & Lookup Table &  & \\
			\hline\hline
			\checkmark & & & 26.84 & 37K\\
			\checkmark & \checkmark & & 18.5 & 65K \\
			\checkmark & \checkmark & \checkmark & 6.08 & 165K \\
			\hline
		\end{tabular}
		\caption{Centralized 23-wPRF benchmarks using different optimization techniques. Packing was done into 64-bit sized words (for both $\Z_2$ and $\Z_3$ vectors). For the lookup table optimization, an $(m=256)$-column matrix was partitioned into 16 slices of 16 columns each. A lookup table containing $81 \times 2^{20}$ elements in $\Z_3$\yuval{Specify units in Runtime column. Memory Usage column: not sure what this means, and in any case the first two rows seem too small to be meaningful (e.g., the amount of memory used by the program for maintaining variables may be bigger). I suggest removing this column, and instead specifying the lookup table size in the caption. Alternatively, we can keep this column and change the title to ``lookup table size'', keeping the first two rows empty. Regarding this size, what does $2^{20}$ count? Bits? Machine words? $\Z_3$ elements? }}
		\label{table:optimization_benchmarks}
	}
	\mahimna{It would be nice to get benchmarks for performance gain using each optimization independently. If possible, it might also be nice to get the numbers for different sized lookup tables. We could also estimate this analytically.}
\end{table}


\paragraph{Distributed wPRF evaluation.}
We consider two 2-party weak PRF protocols. The first protocol is described in \cite{boneh2018-darkmatter} and the second one is our construction as described in \ref{newProtocol}. Since the goal of the implementation was timing the local {\em online} run-time, the implementation used a trusted party to generate the pre-processed variables.

\begin{table}[htbp]
	%[h]
	\begin{center}
		%\begin{minipage}{10cm}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Protocol} & \textbf{Eval/sec} & \textbf{Runtime($\mu$ sec)} \\
			\hline
			\hline
			\textbf{Fully Distributed wPRF~\cite{boneh2018-darkmatter}} & 36K & 28.02  \\
			\hline
			\textbf{Fully Distributed wPRF} &  82K &  12.12\\
			\hline
			
		\end{tabular}
		
		\vspace{-1mm}
		\caption{Optimized protocols runtime. These protocols utilize both the bit-packing and lookup table optimization}
		\label{RuntimeTable}
		%\end{minipage}
	\end{center}
	\vspace{-5mm}
\end{table}

\paragraph{OPRF evaluation.}
In Table~\ref{table:oprf_comparison}, we provide performance benchmarks for our 23-OPRF construction while also comparing it with other constructions. For our timing results, we report both the server and client runtimes (averages over 1000 runs). For each construction, we also include the output size, the size of the preprocessed correlated randomness, and the online communication cost. All constructions are parameterized appropriately to provide 128-bit security.

For the comparison with the DDH-based OPRF construction in~\cite{naor1999-oprf}, we use the libsodium library~\cite{LibSodium} for the elliptic curve scalar multiplication operation. We use the Curve25519 elliptic curve, which has a 256-bit key size, and provides 128 bits of security. \yuval{I actually think that the Naor-Pinkas-Reingold99 paper cited in the TCC '18 paper refers to a different notion of distributing a PRF, namely where the PRF key is distributed between two or more servers and the input is public. Let's try to figure out the source of the simple DDH-based OPRF protocol. Can ask Stas/Hugo if needed.}

\begin{table}[h]
	{
		\centering
		\resizebox{\textwidth}{!}{%
			\begin{tabular}{|cc|c|c|c|c|c|c|c|}%9 columns
				
				\hline
				\multirow{2}{*}{Protocol} & & \multicolumn{2}{c|}{Runtime} & \multirow{2}{*}{Preprocessing} & \multicolumn{2}{c|}{Communication(bits)}\\
				& & Server & Client & &Server & Client\\
				\hline\hline
				\multirow{2}{*}{23-OPRF} & Key Update & 0.65 & - & \multirow{2}{*}{1835} & \multirow{2}{*}{593} & \multirow{2}{*}{512}\\
				& Evaluation & 9.45 & 9.13 & &&\\
				\hline
				DDH-based OPRF & & 57.38& 28.69 & - &256&256\\
				\hline
		\end{tabular}}
		\caption{Comparison of protocols for (semi-honest) OPRF evaluation in the preprocessing model. Runtimes are given in microseconds ($\mu$s). \yuval{What does ``parallel'' mean? Should be explained in the caption. Our output size has roughly 128 bits (81 is the number of $\Z_3$ elements), but it is actually meaningless because the output size can be easily extended or decreased. So let's drop this column. For preprocessing and communication, I suggest using concrete numbers as opposed to general expressions that refer to different values of $n$. }}
		\label{table:oprf_comparison}
	}
	\mahimna{Include more comparisons as necessary}
\end{table}

\newpage
\iffalse
\section{Implementation}
\label{sec:technical_overview}

The pre-shared randomness was generated centrally (as if its generated by a third trusted party). Only the online part was implemented in a distributed manner. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse
\subsection{Representing $Z_2$ vector}

The dark matter function manipulates elements of $Z_2$ and $Z_3$. However, machine words can hold up to 64 bits and we use this to represent many elements in just a few machine words, using bit slicing. Namely, we used each machine word as a vector of 64 bits and applied operations to this bit-vector in a SIMD manner.
Since in our implementation each key $\textbf{K}$  is of size $m \times n = 256 \times 256$ and each input is a vector of size $n = 256$, this results in packing each key o $256 \dfrac 64 = 4$ words and each input vector into 4 words. This may result in time saving of up to $\times 64$ of the run-time.

\subsection{Representing $Z_3$ vector}

The last part of the dark matter function takes an intermediate vector viewed as a vector of $Z_3$ elements, which is then multiplied 
by a matrix $R$ in $Z_3$ to produce the output vector in $Z_3$. We tried different methods for implementing this matrix-vector multiplication over $Z_3$.:	

\subsubsection{Bit slicing} Bit slicing is implemented  by representing a vector over $Z_3$  as two binary vectors - one LSB's and one MSB's. The operations over this $Z_3$ vector - addition, subtraction, multiplication and MUX  - were implemented in a bitwise manner as shown in table \ref{tab:multicol}. We took advantage of the fact that when representing each vector as MSB and LSB, negation (which is the same as multiplying by two) is the same as swapping the most and least significant bits. 
\fi

% TODO: replace with a table - COMPLETED



\begin{table}[ht]
	\caption{Operations in $Z_3$. input:   ${l_1}, {m_1}$ - LSB and MSB  of first trinary number, 
		${l_2}, {m_2}$ - LSB and MSB  of second trinary number,  ${s}$ - selection bit for MUX}
	\begin{center}
		\begin{tabular}{|c|l|}
			\hline
			\textbf{Operations} & \textbf{Methods}\\
			\hline
			\multirow{3}{*}{Addition} & ${t} := ({l_1 \wedge m_2}) \oplus ({l_2 \wedge m_1})$\\
			& $m_{\mathrm{out}} := ( l_1 \wedge  l_2 ) \oplus  t $ \\
			& $l_{\mathrm{out}} :=m_1 \wedge m_2 ) \oplus t $ \\
			\hline
			\multirow{3}{*}{Subtraction} & ${t} := ({l_1} \wedge {l_2}) \oplus ({m_2} \wedge {m_1})$;\\
			& $m_{\mathrm{out}} := (l_1 \wedge m_2 ) \oplus t$;\\
			& $l_{\mathrm{out}} := (m_1 \wedge l_2 ) \oplus t$; \\
			\hline
			\multirow{3}{*}{Multiplication} & $m_{\mathrm{out}}:= (l_1 \vee m_2) \wedge   (m_1 \vee l_2)$; \\
			& $l_{\mathrm{out}} := (l_1 \vee l_2) \wedge     (m_1 \vee m_2)$;\\
			\hline
			\multirow{3}{*}{MUX} & $m_{\mathrm{out}} :=( m_2 \vee s) \wedge (m_1 \vee (\bar{s}) )$; \\
			& $l_{\mathrm{out}}[i] :=( l_2 \vee s) \wedge (l_1 \vee (\bar{s}) )$; \\
			\hline
			
		\end{tabular}
	\end{center}
	\label{tab:multicol}
\end{table}


%TODO: IS THIS CORRECT? CHECK AND CORRECT

\subsubsection{Matrix multiplication using a lookup table}

Even with bitslicing, implementing the matrix-vector multiplication column by column via the opertions from Table~1 is still rather slow.
To get better performence, we capitalized on the fact that the random matrix $R$ is fixed, and we can therefore set up $R$-dependent tables and use table lookups in the implementation of the multiply-by-$R$ operation.

Specifically, we partition the matrix $R$ (which has $m=256$ columns) into sixteen slices of 16 columns each, denoted $R_1,\ldots,R_{16}$.
For each of these small matrices $R_i$, we then build a table with $2^{16}$ entries, holding the result of multiplying $R_i$ by every vector from $\{0,1\}^{16}$.
Namely, let $R_{i,0},R_{i,1},\ldots,R_{i,15}$ be the sixteen columns of the matrix $R_i$.
The table for $R_i$ is then defined as follows: For each index $0\le j < 2^{16}$, let $\vec{J}=(j_0,j_1,\ldots,j_{15})$ be the 0-1 vector holding the bits in the binary representation of~$j$, then we have
\[
T_i[j] = R_i \times \vec{J} = \sum_{k=0}^{15} R_{i,k} \cdot j_k ~~\bmod 3.
\]
Recalling that $R$ is $Z_3$ matrix of dimension $81\times 256$, every entry in each table~$T_i$ therefore holds an 81-vector over $Z_3$.
Specifically, it holds a packed-$Z_3$ element with four words (two for the MSBs and two for the LSBs of this 81-element vector).

Note, however, that $T_i$ can only be used directly to multiply $R_i$ by \emph{0-1 vectors}.
To use $T_i$ when multiplying $R_i$ by a $\{0,1,2\}$ vector, multiply $R_i$ separately by the MBSs and the LSBs of that vector, and then subtract one from the other using the operations from Table~1.
To multiply a dimension-256 $\{0,1,2\}$ vector by the matrix $R$, we partition it into sixteen vectors of dimension 16, use the approach above to multiply each one of them by the corresponding $R_i$, then use the operations from Table~1 to add them all together.
Hence we have

\begin{tabbing}
	\underline{Multiply-by-$R$(input: $\vec{V}\in\{0,1,2\}^{256}$:}\\
	~~1. $Acc := 0$\\
	~~2. For \=$i=0,\ldots,15$\\
	~~3. \> Let $\vec{M}_i,\vec{L}_i\in\{0,1\}^{16}$ be the MSB, LSB vectors (resp.) of $\vec{V}_i=\vec{V}[16i,\ldots,16i+15]$\\
	~~4. \> Set the indexes $m:=\sum_{k=0}^{15}2^i\cdot\vec{M}[i]$
	and $\ell:=\sum_{k=0}^{15}2^i\cdot\vec{L}[i]$\\
	~~5. \> $Acc := Acc + T_i[\ell] - T_i[\ell] \bmod 3$\\
	~~6. Output $Acc$
\end{tabbing}

We note that a slightly faster implementation could be obtained by braking the matrix into (say) 26 slices of upto 10 columns each, and directly identify each 10-vector over $\{0,1,2\}$ with an index $i<3^{10}=59049$.
This implementation would have only 26 lookup operations instead of 32 in the implementation above, so we expect it to be about 20\% faster. On the other hand it would have almost twice the table size of the implementation from above.



\section{Analysis}

Table \ref{CommunicationCosts} includes the computation and communication results for the different protocol.

% Table: Communication Cost
\begin{table}[htbp]
	\label{CommunicationCosts}
	%[h]
	\begin{center}
		%\begin{minipage}{10cm}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\multicolumn{1}{|p{5cm}|}{\centering Protocol}
			& \multicolumn{1}{|p{2cm}|}{\centering Round  \\Complexity}
			& \multicolumn{1}{|p{3cm}|}{\centering Online \\ Communication}
			& \multicolumn{1}{|p{3cm}|}{\centering Preprocessing \\ Size}
			& \multicolumn{1}{|p{3cm}|}{\centering Computation \\ Operation}\\
			\hline
			\hline
			\textbf{Centralized WPRF} & - & - & - & 9K\\
			\hline
			\textbf{Distributed WPRF \cite{boneh2018-darkmatter} } & 4 & 5 & 2K &  45K\\
			\hline
			\textbf{Our WPRF  } & 3 & 5 & 1M &  13K\\
			\hline

			\textbf{Our OPRF } & 3 & 5 & 1M & 11K\\
			%=======
			

			\hline
			
		\end{tabular}
		
		\vspace{-1mm}
		\caption{Communication Analysis of different protocols. The protocols were optimized using both the bit packing and the lookup table}
		\label{CommunicationCosts}
		%\end{minipage}
	\end{center}
	\vspace{-5mm}
\end{table}


\section{Benchmarking}

We compare our run-time to discrete-log based PRFs. To this end, we use the lib sodium library \cite{LibSodium}. The library uses elliptic curve 252 bits, and includes a function that performs scalar multiplication ('crypto\_scalarmult\_ed25519').


\section{Experimental Results}

The system was tested using Ubuntu Server 18.04 on a t2.medium AWS environment. To record the timings, the code was run in a loop 1000 times. Below are run-time results for running a single instance of PRF in microsecond. The results include both centralized and distributed versions of the PRF. In order to increase efficiency, packing and lookup tables were used. The packing indicates both the $Z_2$ and $Z_3$ packing.

%add table
\begin{table}[htbp]
	%[h]
	\begin{center}
		%\begin{minipage}{10cm}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			\textbf{Protocol} & \textbf{Packed }  &  \textbf{lookup} & \textbf{Runs/sec} & \textbf{Runtime($\mu$ sec)}\\
			\hline
			\hline
			\textbf{Centralized weak PRF}  & N  & N  &  50K&20.2 \\
			\hline
			\textbf{Centralized weak PRF} & Y  &  N & 65.4K &18.5  \\
			\hline
			\textbf{Centralized weak PRF} & Y  &  Y & 165K &6.08 \\
			\hline
		\end{tabular}
		
		\vspace{-1mm}
		\caption{Comparison of the run-time and computation of the different Optimized Centralized wPRF.}
		\label{CentralRuntimeTable}
		%\end{minipage}
	\end{center}
	\vspace{-5mm}
\end{table}

\begin{table}[htbp]
	%[h]
	\begin{center}
		%\begin{minipage}{10cm}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Protocol} & \textbf{Runs/sec} & \textbf{Runtime($\mu$ sec)} \\
			\hline
			\hline
			\textbf{Fully Distributed WPRF\cite{boneh2018-darkmatter}} & 24K & 40.56  \\
			\hline
			\textbf{Fully Distributed WPRF} &  82K &  12.12\\
			\hline
			\textbf{OPRF with lookup} &  104K &  9.52 \\
			\hline
			\textbf{Discrete log-based PRF } & 12K &\textbf{86.07}\\
			\hline
			
		\end{tabular}
		
		\vspace{-1mm}
		\caption{Optimized protocols runtime. These protocols utilize both the bit-packing and lookup table optimization}
		\label{RuntimeTable}
		%\end{minipage}
	\end{center}
	\vspace{-5mm}
\end{table}

\fi